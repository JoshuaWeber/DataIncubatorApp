{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Pilgrim\n",
    "## Overview\n",
    "Food Pilgrim is a web application to generate a list of destinations in a specific region that will delight a user. Every city has specific types of food that it is known for. Food Pilgrim will take advantag of this to identify the local specialities and recommend a set of locations to experience them yourself.\n",
    "\n",
    "## Backing Data\n",
    "Primary source of data will be twitter comments. By mining twitter comments we can identify trends in locales about specific types of food. Relying on the assumption that popular local food types will get talked about more frequently on twitter by local users. We will be able to train a system to extract this local specialities from twitter data.\n",
    "\n",
    "## Current Status\n",
    "Initial validation of this project has been completed. A small twitter sample set has been downloaded in 5 sample cities. An analysis was performed to valid the assumption that local special food items will have a high hit rate for local twitter users. Initial progress has been started to explore techniques for creating an automated system to extract local specialities for all cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonpickle\n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup constants for the sample cities\n",
    "cities = ['seattle', 'austin', 'new_orleans', 'chicago', 'philly']\n",
    "filenames = {\n",
    "        'seattle': r'data\\Seattle_Tweets.json',\n",
    "        'austin': r'data\\Austin_Tweets.json',\n",
    "        'new_orleans': r'data\\New_Orleans_Tweets.json',\n",
    "        'chicago': r'data\\Chicago_Tweets.json',\n",
    "        'philly': r'data\\Philly_Tweets.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seattle Data Loaded. 12672 rows\n",
      "austin Data Loaded. 7013 rows\n",
      "new_orleans Data Loaded. 5905 rows\n",
      "chicago Data Loaded. 4553 rows\n",
      "philly Data Loaded. 9870 rows\n",
      "Full dataframe size 40013\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "# This data was collected via the public twitter APIs, via a seperate python script.\n",
    "# Data collection size was heavily limitted by twitter API rate limits and time limits for this Challenege\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for city in cities:\n",
    "    tweets = []\n",
    "    with open(filenames[city], 'r') as f:\n",
    "        for line in f:\n",
    "            tweets.append(jsonpickle.decode(line))\n",
    "        print('{0} Data Loaded. {1} rows'.format(city, len(tweets)))\n",
    "\n",
    "    temp = pd.DataFrame(tweets)\n",
    "    temp = temp.filter(items=['created_at', 'text'])\n",
    "    temp['city'] = city\n",
    "    df = df.append(temp)\n",
    "\n",
    "print('Full dataframe size {0}'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       created_at  \\\n",
      "0  Mon May 06 06:58:50 +0000 2019   \n",
      "1  Mon May 06 06:56:46 +0000 2019   \n",
      "2  Mon May 06 06:56:43 +0000 2019   \n",
      "3  Mon May 06 06:54:20 +0000 2019   \n",
      "4  Mon May 06 06:53:44 +0000 2019   \n",
      "\n",
      "                                                text     city  \n",
      "0  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  \n",
      "1  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  \n",
      "2  how is deconstructed food a thing? just finish...  seattle  \n",
      "3  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  \n",
      "4  Kalain pud ni kian oy.nikaon sila eat all u ca...  seattle  \n"
     ]
    }
   ],
   "source": [
    "# Just a quick sample view of the data.\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We now work to validate a core assumption: That popular food items will have a local specific increase in frequency of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of expected food types and potential text matches for people discussing them.\n",
    "guessWords = {\n",
    "        'gumbo': ['gumbo'],\n",
    "        'teriyaki': ['teriyaki'],\n",
    "        'taco': ['taco'],\n",
    "        'hotdog': ['hot dog', 'hotdog'],\n",
    "        'cheesesteak': ['cheesesteak', 'cheese steak'],\n",
    "        'deepdish': ['deepdish', 'deep dish']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       created_at  \\\n",
      "0  Mon May 06 06:58:50 +0000 2019   \n",
      "1  Mon May 06 06:56:46 +0000 2019   \n",
      "2  Mon May 06 06:56:43 +0000 2019   \n",
      "3  Mon May 06 06:54:20 +0000 2019   \n",
      "4  Mon May 06 06:53:44 +0000 2019   \n",
      "\n",
      "                                                text     city  gumbo  \\\n",
      "0  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  False   \n",
      "1  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  False   \n",
      "2  how is deconstructed food a thing? just finish...  seattle  False   \n",
      "3  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  False   \n",
      "4  Kalain pud ni kian oy.nikaon sila eat all u ca...  seattle  False   \n",
      "\n",
      "   teriyaki   taco  hotdog  cheesesteak  deepdish  \n",
      "0     False  False   False        False     False  \n",
      "1     False  False   False        False     False  \n",
      "2     False  False   False        False     False  \n",
      "3     False  False   False        False     False  \n",
      "4     False  False   False        False     False  \n"
     ]
    }
   ],
   "source": [
    "# Compute new columns that represent if the tweet contained our word targets\n",
    "def containsFoodWord(row, words):\n",
    "    text = row['text'].lower()\n",
    "    for word in words:\n",
    "        if word in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for guess in guessWords:\n",
    "    df[guess] = df.apply(containsFoodWord, args=(guessWords[guess],), axis=1)\n",
    "    \n",
    "# print a peek of the top with the enriched data columns\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an area for lots of added value in the project. We are doing a very simple word match to determine what the tweet was about. It is likely we could get much better performance by analysis tweets with a classified to extract if the tweet was about a food type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             num_tweets  gumbo    gumbo%  teriyaki  teriyaki%   taco  \\\n",
      "city                                                                   \n",
      "austin           7013.0    0.0  0.000000       1.0   0.014259  102.0   \n",
      "chicago          4553.0    2.0  0.043927       0.0   0.000000  142.0   \n",
      "new_orleans      5905.0   29.0  0.491109       1.0   0.016935   24.0   \n",
      "philly           9870.0    1.0  0.010132       1.0   0.010132   51.0   \n",
      "seattle         12672.0    2.0  0.015783      11.0   0.086806  116.0   \n",
      "\n",
      "                taco%  hotdog   hotdog%  cheesesteak  cheesesteak%  deepdish  \\\n",
      "city                                                                           \n",
      "austin       1.454442    12.0  0.171111          2.0      0.028518       0.0   \n",
      "chicago      3.118823    18.0  0.395344          1.0      0.021964       7.0   \n",
      "new_orleans  0.406435     3.0  0.050804          0.0      0.000000       0.0   \n",
      "philly       0.516717   132.0  1.337386         34.0      0.344478       0.0   \n",
      "seattle      0.915404    25.0  0.197285          1.0      0.007891       2.0   \n",
      "\n",
      "             deepdish%  \n",
      "city                    \n",
      "austin        0.000000  \n",
      "chicago       0.153745  \n",
      "new_orleans   0.000000  \n",
      "philly        0.000000  \n",
      "seattle       0.015783  \n"
     ]
    }
   ],
   "source": [
    "# Now we can compute the interesting data. The frequency account by city for our target words.\n",
    "# Aggregate counts of all guessWords by city\n",
    "def agg(x):\n",
    "    data = {'num_tweets': len(x.index)}\n",
    "    for guess in guessWords:\n",
    "        data[guess] = x[guess].sum()\n",
    "        data[guess + '%'] = x[guess].sum() * 100 / len(x.index)\n",
    "    return pd.Series(data)\n",
    "\n",
    "cityCounts = df.groupby('city').apply(agg)\n",
    "\n",
    "print(cityCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! We see fairly strong signals even on this smaller sample set. Especially for **gumbo**, **teriyaki**, **cheesesteak**, and **deepdish**. All items that are traditionally famous as being signature items for their cities. And as seen in tweets are a much higher topic being disucssed in those cities. Not all of our initial guess have worked out. And unexpected results such as the popularity of hot dog in Philidephia were also observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look to validate our second assumption: That a system can be created to identify signature food items for a city from observation of twitter data. The technique used here is a simple frequency count, with the goal to identify uncommon increases in a cities frequency of usage compared to overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of counter 64999\n",
      "[('â€¦', 20989), ('I', 12900), ('food', 12253), ('â€™', 10921), ('eat', 9926), ('foods', 5218), (\"Don't\", 4699), ('eating', 2893), ('2', 2844), ('1', 2838), ('3', 2613), ('What', 2545), ('fried', 2389), ('Healthy', 2344), ('Ramadan', 2340), ('salty', 2292), ('skip', 2281), ('Iftar', 2277), ('overeat', 2275), ('Avoid', 2275), ('Fasting', 2273), ('@AkhSuliee', 2271), ('Suhoor', 2271), ('The', 1992), ('better', 1869), ('restaurant', 1820), ('Food', 1810), ('like', 1803), ('would', 1723), ('us', 1589), ('get', 1459), ('good', 1396), ('\\u200b', 1335), ('people', 1200), ('want', 1194), ('This', 1172), ('live', 1165), ('go', 1159), ('â€œ', 1148), ('place', 1139), ('kids', 1115), ('It', 1098), ('Y', 1070), ('one', 1049), ('check', 1048), ('time', 1032), ('tryna', 1022), ('kind', 984), ('ðŸ˜¤', 975), ('drug', 965)]\n"
     ]
    }
   ],
   "source": [
    "# We use the nltk tokenizer optimized for tweets. And try to filter out common puncuation and stop words.\n",
    "token_count = Counter()\n",
    "tknzr = TweetTokenizer()\n",
    "tweet_count = 0\n",
    "\n",
    "punc = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punc + ['rt', 'RT', 'via', '...']\n",
    "\n",
    "for row in df.itertuples():\n",
    "    tweet = row.text\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    token_count.update(tokens)\n",
    "    tweet_count += 1\n",
    "    #if tweet_count % 1000 == 0:\n",
    "    #    print('Processed {0} tweets'.format(tweet_count) )\n",
    "\n",
    "print('Length of counter {0}'.format(len(token_count)))\n",
    "print(token_count.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial inspection doesn't lead to any great insights. The most common words are not a great indicator for food types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing freq of tokens for seattle\n",
      "Computing freq of tokens for austin\n",
      "Computing freq of tokens for new_orleans\n",
      "Computing freq of tokens for chicago\n",
      "Computing freq of tokens for philly\n"
     ]
    }
   ],
   "source": [
    "# We compute a new dictionary of dictionaries. Holding the per city token frequency count for our sample 5 cities.\n",
    "all_freq = dict()\n",
    "for x in token_count:\n",
    "    #print('x type is {0} and value {1}'.format(type(x), x))\n",
    "    all_freq[x] = token_count[x] / len(token_count)\n",
    "\n",
    "per_city_freq = dict()\n",
    "for city in cities:\n",
    "    print('Computing freq of tokens for {0}'.format(city))\n",
    "    city_data = df[df['city'] == city]\n",
    "    for row in city_data.itertuples():\n",
    "        token_count = Counter()\n",
    "        tweet = row.text\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "        tokens = [token for token in tokens if token not in stop]\n",
    "        token_count.update(tokens)\n",
    "        city_freq = dict()\n",
    "        for x in token_count:\n",
    "            city_freq[x] = token_count[x] / len(token_count)\n",
    "        per_city_freq[city] = city_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@GeneDexter in seattle with prob: 0.12500 compared to total prop: 0.00002\n",
      "dope in seattle with prob: 0.12500 compared to total prop: 0.00015\n",
      "man in seattle with prob: 0.12500 compared to total prop: 0.00631\n",
      "Great in seattle with prob: 0.12500 compared to total prop: 0.00286\n",
      "great in seattle with prob: 0.12500 compared to total prop: 0.00815\n",
      "@carlos_glezgtez in austin with prob: 0.06250 compared to total prop: 0.00012\n",
      "enthusiasts in austin with prob: 0.06250 compared to total prop: 0.00014\n",
      "Austin in austin with prob: 0.06250 compared to total prop: 0.00537\n",
      "can't in austin with prob: 0.06250 compared to total prop: 0.00289\n",
      "miss in austin with prob: 0.06250 compared to total prop: 0.00258\n",
      "2019 in austin with prob: 0.06250 compared to total prop: 0.00340\n",
      "edition in austin with prob: 0.06250 compared to total prop: 0.00020\n",
      "Taste in austin with prob: 0.06250 compared to total prop: 0.00055\n",
      "Mexico in austin with prob: 0.06250 compared to total prop: 0.00065\n",
      "Festival in austin with prob: 0.06250 compared to total prop: 0.00232\n",
      "Wed in austin with prob: 0.06250 compared to total prop: 0.00018\n",
      "1st in austin with prob: 0.06250 compared to total prop: 0.00092\n",
      "@Brazo in austin with prob: 0.06250 compared to total prop: 0.00011\n",
      "wasps in new_orleans with prob: 0.04762 compared to total prop: 0.00005\n",
      "clump in new_orleans with prob: 0.04762 compared to total prop: 0.00005\n",
      "Make in new_orleans with prob: 0.04762 compared to total prop: 0.00200\n",
      "frump in new_orleans with prob: 0.04762 compared to total prop: 0.00005\n",
      "Suck in new_orleans with prob: 0.04762 compared to total prop: 0.00008\n",
      "gas in new_orleans with prob: 0.04762 compared to total prop: 0.00058\n",
      "pump in new_orleans with prob: 0.04762 compared to total prop: 0.00006\n",
      "Slap in new_orleans with prob: 0.04762 compared to total prop: 0.00005\n",
      "Jared in new_orleans with prob: 0.04762 compared to total prop: 0.00008\n",
      "pale in new_orleans with prob: 0.04762 compared to total prop: 0.00006\n",
      "rump in new_orleans with prob: 0.04762 compared to total prop: 0.00005\n",
      "Sit in new_orleans with prob: 0.04762 compared to total prop: 0.00025\n",
      "hard in new_orleans with prob: 0.04762 compared to total prop: 0.00368\n",
      "st in new_orleans with prob: 0.04762 compared to total prop: 0.00063\n",
      "https://t.co/aVr4DJb0HN in new_orleans with prob: 0.04762 compared to total prop: 0.00002\n",
      "@shannonmichele_ in chicago with prob: 0.07692 compared to total prop: 0.00460\n",
      "parsley in chicago with prob: 0.07692 compared to total prop: 0.00466\n",
      "ready in chicago with prob: 0.07692 compared to total prop: 0.00760\n",
      "show in chicago with prob: 0.07692 compared to total prop: 0.00745\n",
      "lobster in chicago with prob: 0.07692 compared to total prop: 0.00485\n",
      "steak in chicago with prob: 0.07692 compared to total prop: 0.00543\n",
      "macaroni in chicago with prob: 0.07692 compared to total prop: 0.00465\n",
      "yams in chicago with prob: 0.07692 compared to total prop: 0.00463\n",
      "cake in chicago with prob: 0.07692 compared to total prop: 0.00697\n",
      "@aubreymichellee in philly with prob: 0.10000 compared to total prop: 0.00002\n",
      "vegan in philly with prob: 0.10000 compared to total prop: 0.00609\n",
      "week in philly with prob: 0.10000 compared to total prop: 0.00595\n",
      "trying in philly with prob: 0.10000 compared to total prop: 0.00398\n",
      "ðŸ¤ª in philly with prob: 0.10000 compared to total prop: 0.00135\n"
     ]
    }
   ],
   "source": [
    "# We now look for insights. We see if any city has an unusually high token count relative to the overall token count.\n",
    "for city in cities:\n",
    "    for token in per_city_freq[city]:\n",
    "        if per_city_freq[city][token] >= 10 * all_freq[token]:\n",
    "            print('{0} in {1} with prob: {2:.5f} compared to total prop: {3:.5f}'.format(token, city, per_city_freq[city][token], all_freq[token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results using this technique are not highly exciting. It is clear that the vast majority of the results are of no help to us in determining popular food for the city. \n",
    "\n",
    "Its clear we are being limitted by the small size of our twitter data set. As local popular items like a link, or usernames are able to cause a trigger to show on this list.\n",
    "\n",
    "There is some small interesting results in this data. Where words like steak, macaroni, yams, and vegan. Came out from this analysis. Potentially showing that with a better feature extraction this could be a useful technique for determining popular food types for a city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
