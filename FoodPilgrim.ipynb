{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Pilgrim\n",
    "## Overview\n",
    "Food Pilgrim is a web application to generate a list of destinations in a specific region that will delight a user. Every city has specific types of food that it is known for. Food Pilgrim will take advantag of this to identify the local specialities and recommend a set of locations to experience them yourself.\n",
    "\n",
    "## Backing Data\n",
    "Primary source of data will be twitter comments. By mining twitter comments we can identify trends in locales about specific types of food. Relying on the assumption that popular local food types will get talked about more frequently on twitter by local users. We will be able to train a system to extract this local specialities from twitter data.\n",
    "\n",
    "## Current Status\n",
    "Initial validation of this project has been completed. A small twitter sample set has been downloaded in 5 sample cities. An analysis was performed to valid the assumption that local special food items will have a high hit rate for local twitter users. Initial progress has been started to explore techniques for creating an automated system to extract local specialities for all cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonpickle\n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup constants for the sample cities\n",
    "cities = ['seattle', 'austin', 'new_orleans', 'chicago', 'philly']\n",
    "filenames = {\n",
    "        'seattle': r'data\\Seattle_Tweets.json',\n",
    "        'austin': r'data\\Austin_Tweets.json',\n",
    "        'new_orleans': r'data\\New_Orleans_Tweets.json',\n",
    "        'chicago': r'data\\Chicago_Tweets.json',\n",
    "        'philly': r'data\\Philly_Tweets.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seattle Data Loaded. 18926 rows\n",
      "austin Data Loaded. 20841 rows\n",
      "new_orleans Data Loaded. 15885 rows\n",
      "chicago Data Loaded. 28697 rows\n",
      "philly Data Loaded. 13559 rows\n",
      "Full dataframe size 97908\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "# This data was collected via the public twitter APIs, via a seperate python script.\n",
    "# Data collection size was heavily limitted by twitter API rate limits and time limits for this Challenege\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for city in cities:\n",
    "    tweets = []\n",
    "    with open(filenames[city], 'r') as f:\n",
    "        for line in f:\n",
    "            tweets.append(jsonpickle.decode(line))\n",
    "        print('{0} Data Loaded. {1} rows'.format(city, len(tweets)))\n",
    "\n",
    "    temp = pd.DataFrame(tweets)\n",
    "    temp = temp.filter(items=['created_at', 'text'])\n",
    "    temp['city'] = city\n",
    "    df = df.append(temp)\n",
    "\n",
    "print('Full dataframe size {0}'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       created_at  \\\n",
      "0  Mon May 06 06:58:50 +0000 2019   \n",
      "1  Mon May 06 06:56:46 +0000 2019   \n",
      "2  Mon May 06 06:56:43 +0000 2019   \n",
      "3  Mon May 06 06:54:20 +0000 2019   \n",
      "4  Mon May 06 06:53:44 +0000 2019   \n",
      "\n",
      "                                                text     city  \n",
      "0  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  \n",
      "1  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  \n",
      "2  how is deconstructed food a thing? just finish...  seattle  \n",
      "3  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  \n",
      "4  Kalain pud ni kian oy.nikaon sila eat all u ca...  seattle  \n"
     ]
    }
   ],
   "source": [
    "# Just a quick sample view of the data.\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We now work to validate a core assumption: That popular food items will have a local specific increase in frequency of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of expected food types and potential text matches for people discussing them.\n",
    "guessWords = {\n",
    "        'gumbo': ['gumbo'],\n",
    "        'teriyaki': ['teriyaki'],\n",
    "        'taco': ['taco'],\n",
    "        'hotdog': ['hot dog', 'hotdog'],\n",
    "        'cheesesteak': ['cheesesteak', 'cheese steak'],\n",
    "        'deepdish': ['deepdish', 'deep dish']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       created_at  \\\n",
      "0  Mon May 06 06:58:50 +0000 2019   \n",
      "1  Mon May 06 06:56:46 +0000 2019   \n",
      "2  Mon May 06 06:56:43 +0000 2019   \n",
      "3  Mon May 06 06:54:20 +0000 2019   \n",
      "4  Mon May 06 06:53:44 +0000 2019   \n",
      "\n",
      "                                                text     city  gumbo  \\\n",
      "0  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  False   \n",
      "1  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  False   \n",
      "2  how is deconstructed food a thing? just finish...  seattle  False   \n",
      "3  RT @AkhSuliee: Healthy Ramadan Fasting:\\n\\n1. ...  seattle  False   \n",
      "4  Kalain pud ni kian oy.nikaon sila eat all u ca...  seattle  False   \n",
      "\n",
      "   teriyaki   taco  hotdog  cheesesteak  deepdish  \n",
      "0     False  False   False        False     False  \n",
      "1     False  False   False        False     False  \n",
      "2     False  False   False        False     False  \n",
      "3     False  False   False        False     False  \n",
      "4     False  False   False        False     False  \n"
     ]
    }
   ],
   "source": [
    "# Compute new columns that represent if the tweet contained our word targets\n",
    "def containsFoodWord(row, words):\n",
    "    text = row['text'].lower()\n",
    "    for word in words:\n",
    "        if word in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for guess in guessWords:\n",
    "    df[guess] = df.apply(containsFoodWord, args=(guessWords[guess],), axis=1)\n",
    "    \n",
    "# print a peek of the top with the enriched data columns\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an area for lots of added value in the project. We are doing a very simple word match to determine what the tweet was about. It is likely we could get much better performance by analysis tweets with a classified to extract if the tweet was about a food type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             num_tweets  gumbo    gumbo%  teriyaki  teriyaki%   taco  \\\n",
      "city                                                                   \n",
      "austin          20841.0    0.0  0.000000       5.0   0.023991  313.0   \n",
      "chicago         28697.0    6.0  0.020908       1.0   0.003485  354.0   \n",
      "new_orleans     15885.0   54.0  0.339943       1.0   0.006295  151.0   \n",
      "philly          13559.0    1.0  0.007375       2.0   0.014750   70.0   \n",
      "seattle         18926.0    2.0  0.010567      13.0   0.068689  206.0   \n",
      "\n",
      "                taco%  hotdog   hotdog%  cheesesteak  cheesesteak%  deepdish  \\\n",
      "city                                                                           \n",
      "austin       1.501847    33.0  0.158342          2.0      0.009596       0.0   \n",
      "chicago      1.233578    70.0  0.243928          1.0      0.003485      40.0   \n",
      "new_orleans  0.950582    12.0  0.075543          0.0      0.000000       0.0   \n",
      "philly       0.516262   144.0  1.062025         42.0      0.309757       0.0   \n",
      "seattle      1.088450    32.0  0.169080          3.0      0.015851       3.0   \n",
      "\n",
      "             deepdish%  \n",
      "city                    \n",
      "austin        0.000000  \n",
      "chicago       0.139387  \n",
      "new_orleans   0.000000  \n",
      "philly        0.000000  \n",
      "seattle       0.015851  \n"
     ]
    }
   ],
   "source": [
    "# Now we can compute the interesting data. The frequency account by city for our target words.\n",
    "# Aggregate counts of all guessWords by city\n",
    "def agg(x):\n",
    "    data = {'num_tweets': len(x.index)}\n",
    "    for guess in guessWords:\n",
    "        data[guess] = x[guess].sum()\n",
    "        data[guess + '%'] = x[guess].sum() * 100 / len(x.index)\n",
    "    return pd.Series(data)\n",
    "\n",
    "cityCounts = df.groupby('city').apply(agg)\n",
    "\n",
    "print(cityCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! We see fairly strong signals even on this smaller sample set. Especially for **gumbo**, **teriyaki**, **cheesesteak**, and **deepdish**. All items that are traditionally famous as being signature items for their cities. And as seen in tweets are a much higher topic being disucssed in those cities. Not all of our initial guess have worked out. And unexpected results such as the popularity of hot dog in Philidephia were also observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look to validate our second assumption: That a system can be created to identify signature food items for a city from observation of twitter data. The technique used here is a simple frequency count, with the goal to identify uncommon increases in a cities frequency of usage compared to overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of counter 107069\n",
      "[('â€¦', 50740), ('I', 34029), ('food', 30508), ('â€™', 29419), ('eat', 24809), ('foods', 6343), ('The', 5796), ('us', 4995), (\"Don't\", 4916), ('like', 4727), ('Food', 4519), ('restaurant', 4502), ('know', 4087), ('What', 3775), ('eating', 3771), ('Shop', 3760), ('Kitty', 3737), ('go', 3644), ('2', 3566), ('get', 3538), ('1', 3392), ('would', 3310), ('â€œ', 3283), ('3', 3146), ('good', 2979), ('This', 2937), ('If', 2787), ('We', 2775), ('better', 2749), ('want', 2729), ('time', 2655), ('Do', 2584), ('â€', 2580), ('fried', 2545), ('day', 2544), ('people', 2515), ('She', 2509), ('today', 2460), ('one', 2439), ('Healthy', 2427), ('Ramadan', 2402), ('skip', 2388), ('special', 2374), ('It', 2364), ('Fasting', 2347), ('salty', 2327), ('You', 2318), ('Avoid', 2312), ('Iftar', 2311), ('ðŸ˜‚', 2308)]\n"
     ]
    }
   ],
   "source": [
    "# We use the nltk tokenizer optimized for tweets. And try to filter out common puncuation and stop words.\n",
    "token_count = Counter()\n",
    "tknzr = TweetTokenizer()\n",
    "tweet_count = 0\n",
    "\n",
    "punc = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punc + ['rt', 'RT', 'via', '...']\n",
    "\n",
    "for row in df.itertuples():\n",
    "    tweet = row.text\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    token_count.update(tokens)\n",
    "    tweet_count += 1\n",
    "    #if tweet_count % 1000 == 0:\n",
    "    #    print('Processed {0} tweets'.format(tweet_count) )\n",
    "\n",
    "print('Length of counter {0}'.format(len(token_count)))\n",
    "print(token_count.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial inspection doesn't lead to any great insights. The most common words are not a great indicator for food types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing freq of tokens for seattle\n",
      "Computing freq of tokens for austin\n",
      "Computing freq of tokens for new_orleans\n",
      "Computing freq of tokens for chicago\n",
      "Computing freq of tokens for philly\n",
      "Computing inverse freq of tokens for seattle\n",
      "Computing inverse freq of tokens for austin\n",
      "Computing inverse freq of tokens for new_orleans\n",
      "Computing inverse freq of tokens for chicago\n",
      "Computing inverse freq of tokens for philly\n"
     ]
    }
   ],
   "source": [
    "# We compute a new dictionary with the token frequency count for our sample 5 cities.\n",
    "all_freq = dict()\n",
    "token_count = Counter()\n",
    "for row in df.itertuples():\n",
    "    tweet = row.text\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    token_count.update(tokens)\n",
    "for x in token_count:\n",
    "    all_freq[x] = token_count[x] / df.shape[0]\n",
    "\n",
    "# Compute the freq of each token for just 1 city\n",
    "per_city_freq = dict()\n",
    "for city in cities:\n",
    "    print('Computing freq of tokens for {0}'.format(city))\n",
    "    city_data = df[df['city'] == city]\n",
    "    token_count = Counter()\n",
    "    city_freq = dict()\n",
    "    for row in city_data.itertuples():\n",
    "        tweet = row.text\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "        tokens = [token for token in tokens if token not in stop]\n",
    "        token_count.update(tokens)\n",
    "    for x in token_count:\n",
    "        city_freq[x] = token_count[x] / city_data.shape[0]\n",
    "    per_city_freq[city] = city_freq\n",
    "    \n",
    "# Compute the freq of a token with 1 excluded city\n",
    "not_per_city_freq = dict()\n",
    "for city in cities:\n",
    "    print('Computing inverse freq of tokens for {0}'.format(city))\n",
    "    not_city_data = df[df['city'] != city]\n",
    "    not_token_count = Counter()\n",
    "    not_city_freq = dict()\n",
    "    for row in not_city_data.itertuples():\n",
    "        tweet = row.text\n",
    "        tokens = tknzr.tokenize(tweet)\n",
    "        tokens = [token for token in tokens if token not in stop]\n",
    "        not_token_count.update(tokens)\n",
    "    for x in not_token_count:\n",
    "        not_city_freq[x] = not_token_count[x] / not_city_data.shape[0]\n",
    "    not_per_city_freq[city] = city_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AkhSuliee in seattle with prob: 0.12042 compared to total prop: 0.02328\n",
      "Fasting in seattle with prob: 0.12042 compared to total prop: 0.02397\n",
      "Suhoor in seattle with prob: 0.12042 compared to total prop: 0.02328\n",
      "overeat in seattle with prob: 0.12057 compared to total prop: 0.02339\n",
      "Iftar in seattle with prob: 0.12063 compared to total prop: 0.02360\n",
      "Avoid in seattle with prob: 0.12057 compared to total prop: 0.02361\n",
      "salty in seattle with prob: 0.12052 compared to total prop: 0.02377\n",
      "@leetaevong in seattle with prob: 0.00682 compared to total prop: 0.00132\n",
      "Yuta in seattle with prob: 0.00687 compared to total prop: 0.00133\n",
      "Taeyong in seattle with prob: 0.00687 compared to total prop: 0.00133\n",
      "Jaehyun in seattle with prob: 0.00682 compared to total prop: 0.00132\n",
      "â€‹ in seattle with prob: 0.09754 compared to total prop: 0.01929\n",
      "@KylePlantEmoji in seattle with prob: 0.03387 compared to total prop: 0.00657\n",
      "https://t.co/AzV5HtsnjX in seattle with prob: 0.00793 compared to total prop: 0.00153\n",
      "Yzma in seattle with prob: 0.00655 compared to total prop: 0.00127\n",
      "Seattle in seattle with prob: 0.02320 compared to total prop: 0.00455\n",
      "https://t.co/U2QmJ5oS1i in seattle with prob: 0.01627 compared to total prop: 0.00315\n",
      "graduate in seattle with prob: 0.01765 compared to total prop: 0.00348\n",
      "https://t.co/yZSQTIa8Sk in seattle with prob: 0.00771 compared to total prop: 0.00149\n",
      "@tough_and_rich in seattle with prob: 0.01178 compared to total prop: 0.00228\n",
      "https://t.co/dLaFVH7J0M in seattle with prob: 0.01184 compared to total prop: 0.00229\n",
      "@itadaki_yasu in seattle with prob: 0.00608 compared to total prop: 0.00117\n",
      "5G in seattle with prob: 0.00528 compared to total prop: 0.00104\n",
      ". .. in seattle with prob: 0.00539 compared to total prop: 0.00106\n",
      "QQ in seattle with prob: 0.00539 compared to total prop: 0.00106\n",
      "G51 in seattle with prob: 0.00534 compared to total prop: 0.00103\n",
      "Everybody in new_orleans with prob: 0.06100 compared to total prop: 0.01016\n",
      "@Vonnieradass in new_orleans with prob: 0.07693 compared to total prop: 0.01248\n",
      "check in new_orleans with prob: 0.07926 compared to total prop: 0.01556\n",
      "tryna in new_orleans with prob: 0.08052 compared to total prop: 0.01449\n",
      "drug in new_orleans with prob: 0.07699 compared to total prop: 0.01285\n",
      "ðŸ˜¤ in new_orleans with prob: 0.07806 compared to total prop: 0.01323\n",
      "https://t.co/BK5ZJhpdvv in new_orleans with prob: 0.07680 compared to total prop: 0.01246\n",
      "nobody in new_orleans with prob: 0.03324 compared to total prop: 0.00599\n",
      "#NewOrleans in new_orleans with prob: 0.00661 compared to total prop: 0.00110\n",
      "@subtweetshawn0 in new_orleans with prob: 0.03179 compared to total prop: 0.00516\n",
      "Orleans in new_orleans with prob: 0.03557 compared to total prop: 0.00633\n",
      "Jazz in new_orleans with prob: 0.00743 compared to total prop: 0.00128\n",
      "NOLA in new_orleans with prob: 0.00812 compared to total prop: 0.00139\n",
      "Louisiana in new_orleans with prob: 0.00856 compared to total prop: 0.00143\n",
      "@darrelwilliams_ in new_orleans with prob: 0.00944 compared to total prop: 0.00153\n",
      "Soooooo in new_orleans with prob: 0.00951 compared to total prop: 0.00155\n",
      "wing in new_orleans with prob: 0.00988 compared to total prop: 0.00185\n",
      "@scottsantens in new_orleans with prob: 0.01360 compared to total prop: 0.00223\n",
      "horrified in new_orleans with prob: 0.00598 compared to total prop: 0.00106\n",
      "forcing in new_orleans with prob: 0.00604 compared to total prop: 0.00116\n",
      "ðŸ˜‘ in new_orleans with prob: 0.03198 compared to total prop: 0.00559\n",
      "@TPrice504 in new_orleans with prob: 0.01221 compared to total prop: 0.00198\n",
      "hire in new_orleans with prob: 0.00648 compared to total prop: 0.00123\n",
      "experienced in new_orleans with prob: 0.00636 compared to total prop: 0.00125\n",
      "journalist in new_orleans with prob: 0.00604 compared to total prop: 0.00101\n",
      "Email in new_orleans with prob: 0.00604 compared to total prop: 0.00108\n",
      "@Marcmywords2 in new_orleans with prob: 0.00837 compared to total prop: 0.00136\n",
      "@NOLAnews in new_orleans with prob: 0.01171 compared to total prop: 0.00190\n",
      "WHILE in new_orleans with prob: 0.00825 compared to total prop: 0.00138\n",
      "@_fournette in new_orleans with prob: 0.05999 compared to total prop: 0.00973\n",
      "harder in new_orleans with prob: 0.05999 compared to total prop: 0.01031\n",
      "satisfied in new_orleans with prob: 0.06050 compared to total prop: 0.00994\n",
      "https://t.co/hfbWHfJghw in new_orleans with prob: 0.03135 compared to total prop: 0.00509\n",
      "sports in new_orleans with prob: 0.01681 compared to total prop: 0.00321\n",
      "inventor in new_orleans with prob: 0.00818 compared to total prop: 0.00133\n",
      "bowli in new_orleans with prob: 0.00806 compared to total prop: 0.00131\n",
      "Bud in new_orleans with prob: 0.00711 compared to total prop: 0.00122\n",
      "@KeeangaYamahtta in philly with prob: 0.06586 compared to total prop: 0.00912\n",
      "kind in philly with prob: 0.06866 compared to total prop: 0.01154\n",
      "demented in philly with prob: 0.06586 compared to total prop: 0.00914\n",
      "prevent in philly with prob: 0.06682 compared to total prop: 0.00956\n",
      "graduating in philly with prob: 0.06593 compared to total prop: 0.00925\n",
      "lunchroom in philly with prob: 0.06593 compared to total prop: 0.00935\n",
      "tab in philly with prob: 0.06616 compared to total prop: 0.00920\n",
      "kin in philly with prob: 0.06579 compared to total prop: 0.00916\n",
      "Philadelphia in philly with prob: 0.01121 compared to total prop: 0.00172\n",
      "PA in philly with prob: 0.00693 compared to total prop: 0.00104\n",
      "@saragoldrickrab in philly with prob: 0.02773 compared to total prop: 0.00388\n",
      "@hope4college in philly with prob: 0.02294 compared to total prop: 0.00318\n",
      "insecurity in philly with prob: 0.02567 compared to total prop: 0.00478\n",
      "Embiid in philly with prob: 0.00723 compared to total prop: 0.00100\n",
      "Philly in philly with prob: 0.01726 compared to total prop: 0.00250\n",
      "4th in philly with prob: 0.02095 compared to total prop: 0.00329\n",
      "#RealCollege in philly with prob: 0.01954 compared to total prop: 0.00271\n",
      "survey in philly with prob: 0.01187 compared to total prop: 0.00176\n",
      "report in philly with prob: 0.02072 compared to total prop: 0.00381\n",
      "campus in philly with prob: 0.01674 compared to total prop: 0.00305\n",
      "housing in philly with prob: 0.01630 compared to total prop: 0.00268\n",
      "Based in philly with prob: 0.01084 compared to total prop: 0.00168\n",
      "resu in philly with prob: 0.01077 compared to total prop: 0.00149\n",
      "Butter in philly with prob: 0.00767 compared to total prop: 0.00136\n",
      "prison in philly with prob: 0.00981 compared to total prop: 0.00171\n",
      "@Phillies in philly with prob: 0.00944 compared to total prop: 0.00131\n",
      "@libertysoap in philly with prob: 0.01335 compared to total prop: 0.00185\n",
      "Soap in philly with prob: 0.06822 compared to total prop: 0.00946\n",
      "Hand in philly with prob: 0.00811 compared to total prop: 0.00130\n",
      "Crafted in philly with prob: 0.00738 compared to total prop: 0.00102\n",
      "Natural in philly with prob: 0.00804 compared to total prop: 0.00155\n",
      "Scent in philly with prob: 0.00804 compared to total prop: 0.00111\n",
      "Students in philly with prob: 0.01114 compared to total prop: 0.00184\n",
      "national in philly with prob: 0.00686 compared to total prop: 0.00133\n",
      "Large in philly with prob: 0.01217 compared to total prop: 0.00172\n",
      "Cold in philly with prob: 0.00856 compared to total prop: 0.00143\n",
      "ðŸ’‹ in philly with prob: 0.00804 compared to total prop: 0.00123\n",
      "Yesterday in philly with prob: 0.00649 compared to total prop: 0.00123\n",
      "released in philly with prob: 0.00679 compared to total prop: 0.00110\n",
      "political in philly with prob: 0.00974 compared to total prop: 0.00175\n",
      "DO in philly with prob: 0.00885 compared to total prop: 0.00170\n",
      "poll in philly with prob: 0.00841 compared to total prop: 0.00125\n",
      "majority in philly with prob: 0.00870 compared to total prop: 0.00135\n",
      "59 in philly with prob: 0.00833 compared to total prop: 0.00117\n",
      "@O4Sswarthmore in philly with prob: 0.01623 compared to total prop: 0.00225\n",
      "Dean in philly with prob: 0.00833 compared to total prop: 0.00133\n",
      "Jim in philly with prob: 0.00568 compared to total prop: 0.00104\n",
      "bathroom in philly with prob: 0.00974 compared to total prop: 0.00185\n",
      "@CaitJTaylor in philly with prob: 0.00951 compared to total prop: 0.00132\n",
      "Less in philly with prob: 0.00959 compared to total prop: 0.00166\n",
      "beds in philly with prob: 0.00966 compared to total prop: 0.00141\n",
      "#privateprisons in philly with prob: 0.00959 compared to total prop: 0.00133\n",
      "Contrary in philly with prob: 0.00959 compared to total prop: 0.00134\n",
      "so-called in philly with prob: 0.00959 compared to total prop: 0.00133\n",
      "progressive in philly with prob: 0.00966 compared to total prop: 0.00145\n",
      "lea in philly with prob: 0.00951 compared to total prop: 0.00141\n",
      "draw in philly with prob: 0.00870 compared to total prop: 0.00131\n",
      "@patmakesart in philly with prob: 0.00826 compared to total prop: 0.00114\n",
      "@egoraptor in philly with prob: 0.00833 compared to total prop: 0.00115\n",
      "inedible in philly with prob: 0.00833 compared to total prop: 0.00118\n",
      "concerning in philly with prob: 0.00833 compared to total prop: 0.00122\n"
     ]
    }
   ],
   "source": [
    "# We now look for insights. We see if any city has an unusually high token count relative to the overall token count.\n",
    "for city in cities:\n",
    "    for token in per_city_freq[city]: \n",
    "        if (per_city_freq[city][token] >=  5 * all_freq[token]) and (all_freq[token] >= 0.001):\n",
    "            print('{0} in {1} with prob: {2:.5f} compared to total prop: {3:.5f}'.format(token, city, per_city_freq[city][token], all_freq[token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results using this technique are not highly exciting. It is clear that the vast majority of the results are of no help to us in determining popular food for the city. \n",
    "\n",
    "Its clear we are being limitted by the small size of our twitter data set. As local popular items like a link, or usernames are able to cause a trigger to show on this list.\n",
    "\n",
    "There is some small interesting results in this data. Where words like steak, macaroni, yams, and vegan. Came out from this analysis. Potentially showing that with a better feature extraction this could be a useful technique for determining popular food types for a city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
